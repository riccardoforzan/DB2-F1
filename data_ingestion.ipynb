{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, RDFS\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the path in which the notebook is executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = str(Path(os.path.abspath(os.getcwd())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize external ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "W3ID_PERSON = Namespace(\"https://w3id.org/MON/person.owl#\")\n",
    "DBPEDIA_F1 = Namespace(\"https://dbpedia.org/ontology/FormulaOneTeam#\")\n",
    "SCHEMA_CITY = Namespace(\"http://schema.org/City#\")\n",
    "SCHEMA_ORG = Namespace(\"http://schema.org/\")\n",
    "F1 = Namespace(\"http://www.dei.unipd.it/database2/Formula1Ontology#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV files that maps between denominations and country.\n",
    "The content of these files is used to map between nationality to country and from country to the 2 digits ISO code representing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom_csv = base_path + '/data/utils/denom.csv'\n",
    "denom_df = pd.read_csv(denom_csv, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_csv = base_path + '/data/utils/iso_country_codes.csv'\n",
    "country_codes_df = pd.read_csv(country_codes_csv, sep=',')\n",
    "country_codes_df['alpha-2'] = country_codes_df['alpha-2'].str.lower() #convert country codes to lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import circuit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"cities\", SCHEMA_CITY)\n",
    "g.bind(\"schema_org\", SCHEMA_ORG)\n",
    "g.bind(\"f1\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_csv = base_path + '/data/circuits.csv'\n",
    "circuits_df = pd.read_csv(circuits_csv, sep=',')\n",
    "circuits_df = circuits_df.replace({np.nan: None, '\\\\N': None})\n",
    "circuits_df['location_real_name'] = circuits_df['location']\n",
    "circuits_df['location'] = circuits_df['location'].str.replace(' ','')     #remove white spaces from city names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the circuit's data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in circuits_df.iterrows():\n",
    "\n",
    "    circuit_id = \"circuit\"+str(row['circuitId'])    # create a unique identifier for the circuit\n",
    "    Circuit = URIRef(F1[circuit_id])                # create the circuit\n",
    "\n",
    "    g.add((Circuit, RDF.type, F1.Circuit))\n",
    "    g.add((Circuit, F1['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "\n",
    "    #add altitude info\n",
    "    if not row['alt'] == None:\n",
    "        g.add((Circuit, F1['altitude'], Literal(row['alt'], datatype=XSD.int)))\n",
    "\n",
    "    #add latitude info\n",
    "    g.add((Circuit, F1['latitude'], Literal(row['lat'], datatype=XSD.decimal)))\n",
    "\n",
    "    #add longitude info\n",
    "    g.add((Circuit, F1['longitude'], Literal(row['lng'], datatype=XSD.decimal)))\n",
    "\n",
    "    #add circuit city\n",
    "    City = URIRef(SCHEMA_CITY[row[\"location\"]])\n",
    "    g.add((City, SCHEMA_ORG['name'], Literal(row['location_real_name'], datatype=XSD.string)))  \n",
    "    g.add((Circuit, F1['hasCity'], City))\n",
    "\n",
    "\n",
    "    # Convert the country to the associated alpha-2 code\n",
    "    country = country_codes_df.loc[country_codes_df['name'] == row['country']]\n",
    "\n",
    "    if not country.empty:\n",
    "        country_code = country['alpha-2'].iloc[0]\n",
    "        Country = URIRef(CNS[country_code])\n",
    "        g.add((Circuit, F1['hasCountry'], Country))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the serialized graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/circuits.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import constructor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"f1\", F1)\n",
    "g.bind(\"dbpedia_f1\", DBPEDIA_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructors_csv = base_path + '/data/constructors.csv'\n",
    "constructors_df = pd.read_csv(constructors_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load constructor participation: associate constructor nationality to country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructors_df = pd.merge(constructors_df, denom_df, on='nationality', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load constructor's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_standings_csv = base_path + '/data/constructor_standings.csv'\n",
    "constructor_standings_df = pd.read_csv(constructor_standings_csv, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_results_csv = base_path + '/data/constructor_results.csv'\n",
    "constructor_results_df = pd.read_csv(constructor_results_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the constructor's data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "for const_i, const_row in constructors_df.iterrows():\n",
    "    constructor_id = const_row['constructorId']\n",
    "    constructor_token = \"constructor\"+str(constructor_id)   #unique identifier for the constructor\n",
    "    FormulaOneTeam = URIRef(DBPEDIA_F1[constructor_token])\n",
    "    \n",
    "    g.add((FormulaOneTeam, RDF.type, DBPEDIA_F1.FormulaOneTeam))\n",
    "    g.add((FormulaOneTeam, F1['name'], Literal(const_row['name'], datatype=XSD.string)))\n",
    "\n",
    "    # Convert the country to the associated alpha-2 code\n",
    "    country = country_codes_df.loc[country_codes_df['name'] == const_row['country']]\n",
    "\n",
    "    if not country.empty:\n",
    "        country_code = country['alpha-2'].iloc[0]\n",
    "        Country = URIRef(CNS[country_code])\n",
    "        g.add((FormulaOneTeam, F1['hasCountry'], Country))\n",
    "\n",
    "    # Find all the races in which a constructor has participated\n",
    "    participated_races_df = constructor_results_df.loc[constructor_results_df['constructorId'] == constructor_id]\n",
    "\n",
    "    for part_id, part_row in participated_races_df.iterrows():\n",
    "        part_token = \"participation\"+str(part_id+1)     #unique identifier for the participation\n",
    "        Participate = URIRef(F1[part_token])\n",
    "\n",
    "        g.add((Participate, RDF.type, F1.Participate))\n",
    "\n",
    "        #get data from constructor standings for the given race\n",
    "        constructor_standing = constructor_standings_df.loc[(constructor_standings_df['raceId'] == part_row['raceId']) & (constructor_standings_df['constructorId'] == constructor_id)]\n",
    "\n",
    "        if not constructor_standing.empty:\n",
    "            g.add((Participate, F1['cp_points_after_race'], Literal(constructor_standing['points'].iloc[0], datatype=XSD.decimal)))\n",
    "            g.add((Participate, F1['cp_position_after_race'], Literal(constructor_standing['position'].iloc[0], datatype=XSD.int)))\n",
    "            g.add((Participate, F1['number_of_wins_after_race'], Literal(constructor_standing['wins'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "        #add the race_weekend associated to the drive\n",
    "        race_weekend_token = \"raceWeekEnd\"+str(part_row['raceId'])\n",
    "        g.add((Participate, F1['during'], URIRef(F1[race_weekend_token])))\n",
    "\n",
    "        #add the drive\n",
    "        g.add((FormulaOneTeam, F1['participateIn'], Participate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/constructors.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import driver data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph and bind the ontologies namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"person\", W3ID_PERSON)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"f1\", F1)\n",
    "g.bind(\"dbpedia_f1\", DBPEDIA_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_csv = base_path + '/data/drivers.csv'                               \n",
    "driver_df = pd.read_csv(drivers_csv, sep=',')\n",
    "driver_df = driver_df.replace({np.nan: None, '\\\\N': None})\n",
    "driver_df = pd.merge(driver_df, denom_df, on='nationality', how='inner')    #associate the nationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers race results from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv = base_path + '/data/results.csv'\n",
    "results_df = pd.read_csv(results_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers qualifying results from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying_csv = base_path + '/data/qualifying.csv'\n",
    "qualifying_df = pd.read_csv(qualifying_csv, sep=',')\n",
    "qualifying_df = qualifying_df.replace({np.nan: None, '\\\\N': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers sprint race results from the csv file (Warning: not all the race week ends have a sprint race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprint_races_csv = base_path + '/data/sprint_results.csv'\n",
    "sprint_races_df = pd.read_csv(sprint_races_csv, sep=',')\n",
    "sprint_races_df = sprint_races_df.replace({np.nan: None, '\\\\N': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the driver championship standing from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_standings_csv = base_path + '/data/driver_standings.csv'\n",
    "driver_standings_df = pd.read_csv(driver_standings_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the race status from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_csv = base_path + '/data/status.csv'\n",
    "status_df = pd.read_csv(status_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data about laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_times_csv = base_path + '/data/lap_times.csv'\n",
    "lap_times_df = pd.read_csv(lap_times_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data about pit stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_stops_csv = base_path + '/data/pit_stops.csv'\n",
    "pit_stops_df = pd.read_csv(pit_stops_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create the drivers and associate all the informations\n",
    "2) Create the drives related to the drivers and associate all the informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using a compiled regex matches string of type MM:SS.FFFF that represents lap times.\n",
    "If no match is possible returns NaN\n",
    "'''\n",
    "\n",
    "time_tokenizer = re.compile(\"[: .]\")\n",
    "def str_time_to_decimal(time: str) -> float:\n",
    "    tokens = time_tokenizer.split(time)\n",
    "\n",
    "    if len(tokens) != 3: \n",
    "        return float('NaN')\n",
    "        \n",
    "    time = 0.0\n",
    "    time += int(tokens[0]) * 60 + float(tokens[1]) + float(\"0.\" + tokens[2])\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "for driver_index, driver_row in driver_df.iterrows():\n",
    "\n",
    "    driverId = driver_row['driverId']\n",
    "    uriId = \"driver\"+str(driverId)\n",
    "\n",
    "    Driver = URIRef(F1[uriId])\n",
    "    \n",
    "    g.add((Driver, RDF.type, F1.Driver))\n",
    "    \n",
    "    #Add first_name, last_name and birth_date (Person data properties) to Driver\n",
    "    g.add((Driver, W3ID_PERSON['firstName'], Literal(driver_row['forename'], datatype=XSD.string)))\n",
    "    g.add((Driver, W3ID_PERSON['lastName'], Literal(driver_row['surname'], datatype=XSD.string)))\n",
    "    g.add((Driver, W3ID_PERSON['birthDate'], Literal(driver_row['dob'], datatype=XSD.date)))\n",
    "\n",
    "    #Add driver number and code\n",
    "    if driver_row['number'] != None:\n",
    "        g.add((Driver, F1['number'], Literal(driver_row['number'], datatype=XSD.int)))\n",
    "    \n",
    "    if driver_row['code'] != None:\n",
    "        g.add((Driver, F1['code'], Literal(driver_row['code'], datatype=XSD.string)))       \n",
    "\n",
    "    #Add driver country\n",
    "\n",
    "    # Convert the country to the associated alpha-2 code\n",
    "    country = country_codes_df.loc[country_codes_df['name'] == driver_row['country']]\n",
    "\n",
    "    if not country.empty:\n",
    "        country_code = country['alpha-2'].iloc[0]\n",
    "        Country = URIRef(CNS[country_code])\n",
    "        g.add((Driver, F1['nationality'], Country))\n",
    "\n",
    "    #get all the race, qualifying, sprint race results of the given driver, these are the driver's drives\n",
    "    driverDrives = results_df.loc[results_df['driverId'] == driverId] \n",
    "\n",
    "    #add the race status information\n",
    "    driverDrives = pd.merge(driverDrives, status_df, on='statusId', how='inner')\n",
    "\n",
    "    for drive_i, drive_row in driverDrives.iterrows():\n",
    "        driveId = drive_row['resultId']\n",
    "        \n",
    "        uriDriveId = \"drive\"+str(driveId)\n",
    "        \n",
    "        #create the Drive object, a Drive contains all the race weekend performance information of a given driver\n",
    "        Drive = URIRef(F1[uriDriveId])\n",
    "\n",
    "        g.add((Drive, RDF.type, F1.Drive))\n",
    "\n",
    "        #id of the constructor for which the driver drives\n",
    "        idCons = \"constructor\"+str(drive_row['constructorId'])\n",
    "        g.add((Drive, F1['driveFor'], URIRef(DBPEDIA_F1[idCons])))\n",
    "\n",
    "        g.add((Drive, F1['race_position'], Literal(drive_row['positionOrder'], datatype=XSD.int))) \n",
    "            \n",
    "        #add race status\n",
    "        g.add((Drive, F1['status'], Literal(drive_row['status'], datatype=XSD.string)))\n",
    "\n",
    "        #searching the qualify of the given driver in the given drive \n",
    "        qualifyDriver = qualifying_df.loc[(qualifying_df['driverId'] == driverId) & (qualifying_df['raceId'] == drive_row['raceId'])]\n",
    "\n",
    "        if not qualifyDriver.empty:\n",
    "            g.add((Drive, F1['quali_position'], Literal(qualifyDriver['position'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "            #check for qualify times\n",
    "            if qualifyDriver['q1'].iloc[0] != None:\n",
    "                time = str_time_to_decimal(qualifyDriver['q1'].iloc[0])\n",
    "                g.add((Drive, F1['q1_time'], Literal(time, datatype=XSD.decimal)))\n",
    "            \n",
    "            if qualifyDriver['q2'].iloc[0] != None:\n",
    "                time = str_time_to_decimal(qualifyDriver['q2'].iloc[0])\n",
    "                g.add((Drive, F1['q2_time'], Literal(time, datatype=XSD.decimal)))\n",
    "\n",
    "            if qualifyDriver['q3'].iloc[0] != None:\n",
    "                time = str_time_to_decimal(qualifyDriver['q3'].iloc[0])\n",
    "                g.add((Drive, F1['q3_time'], Literal(time, datatype=XSD.decimal)))\n",
    "\n",
    "        #searching the sprint_race position of the given driver in the given drive if present\n",
    "        sprintDriver = sprint_races_df.loc[(sprint_races_df['driverId'] == driverId) & (sprint_races_df['raceId'] == driveId)]\n",
    "\n",
    "        #check for sprint_race position\n",
    "        if not sprintDriver.empty:\n",
    "            g.add((Drive, F1['sprint_position'], Literal(sprintDriver['position'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "        #get data from constructor standings for the given race\n",
    "        driver_standing = driver_standings_df.loc[(driver_standings_df['raceId'] == drive_row['raceId']) & (driver_standings_df['driverId'] == driverId)]\n",
    "\n",
    "        if not driver_standing.empty:\n",
    "            g.add((Drive, F1['cp_points_after_race'], Literal(driver_standing['points'].iloc[0], datatype=XSD.decimal )))\n",
    "            g.add((Drive, F1['cp_position_after_race'], Literal(driver_standing['position'].iloc[0], datatype=XSD.int)))\n",
    "            g.add((Drive, F1['number_of_wins'], Literal(driver_standing['wins'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "        # Get info about race laps\n",
    "        lap_times = lap_times_df.loc[(lap_times_df['raceId'] == drive_row['raceId']) & (lap_times_df['driverId'] == driverId)]\n",
    "\n",
    "        if not lap_times.empty:\n",
    "            best = lap_times['milliseconds'].min() / 1000\n",
    "            g.add((Drive, F1['race_fastest_lap'], Literal(best, datatype=XSD.decimal)))\n",
    "        \n",
    "        # Get info about pit stops\n",
    "        pit_stops = pit_stops_df.loc[(pit_stops_df['raceId'] == drive_row['raceId']) & (pit_stops_df['driverId'] == driverId)]\n",
    "        if not pit_stops.empty:\n",
    "            pit_stop_count = pit_stops.shape[0]\n",
    "            best = lap_times['milliseconds'].min() / 1000\n",
    "            worst = lap_times['milliseconds'].max() / 1000\n",
    "            g.add((Drive, F1['pitstop_count'], Literal(pit_stop_count, datatype=XSD.int)))\n",
    "            g.add((Drive, F1['fastest_pitstop'], Literal(best, datatype=XSD.decimal)))\n",
    "            g.add((Drive, F1['longest_pitstop'], Literal(worst, datatype=XSD.decimal)))\n",
    "\n",
    "        #add the race_weekend associated to the drive\n",
    "        idRWE = \"raceWeekEnd\"+str(drive_row['raceId'])\n",
    "        g.add((Drive, F1['during'], URIRef(F1[idRWE])))  \n",
    "\n",
    "        #add the drive\n",
    "        g.add((Driver, F1['hasDrivenIn'], Drive))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/drivers.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Race Weekends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"f1\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the race weekends from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_csv = base_path + '/data/races.csv'   #Read the races\n",
    "races_df = pd.read_csv(races_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the fans rating for the race weekends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_ratings_csv = base_path + '/data/fan_ratings.csv'   #Read the races\n",
    "fan_ratings_df = pd.read_csv(fan_ratings_csv, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to be able to merge\n",
    "fan_ratings_df.rename(columns={'Y': 'year', 'R': 'round','RATING':'rating'}, inplace=True)\n",
    "fan_rated_races = pd.merge(races_df, fan_ratings_df, how='left', on=['year', 'round'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for race_index, race_row in fan_rated_races.iterrows():\n",
    "    race_weekend_token = \"raceWeekEnd\"+str(race_row['raceId'])\n",
    "    Race = URIRef(F1[race_weekend_token])\n",
    "    \n",
    "    g.add((Race, RDF.type, F1.RaceWeekend))\n",
    "    g.add((Race, F1['name'], Literal(race_row['name'], datatype=XSD.string))) \n",
    "    g.add((Race, F1['year'], Literal(race_row['year'], datatype=XSD.int)))\n",
    "    g.add((Race, F1['round'], Literal(race_row['round'], datatype=XSD.int)))\n",
    "    g.add((Race, F1['date'], Literal(race_row['date'], datatype=XSD.date)))\n",
    "\n",
    "    if not np.isnan(race_row['rating']):\n",
    "        g.add((Race, F1['fans_rating'], Literal(race_row['rating'], datatype=XSD.float)))\n",
    "\n",
    "    #create circuit node\n",
    "    circuit_token = 'circuit' + str(race_row['circuitId'])\n",
    "    Circuit = URIRef(F1[circuit_token])\n",
    "\n",
    "    g.add((Race, F1['takePlaceIn'], Circuit))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/raceWeekend.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('db2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019df42223066dde4157690601c8100b607ef52bb43ff483069c00805dbb71b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
