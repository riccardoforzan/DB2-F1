{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, RDFS\n",
    "from rdflib.namespace import FOAF, XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the path in which the notebook is executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = str(Path(os.path.abspath(os.getcwd())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize external ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "W3ID_PERSON = Namespace(\"https://w3id.org/MON/person.owl#Person\")\n",
    "DBPEDIA_F1 = Namespace(\"http://dbpedia.org/resource/FormulaOneTeam#\")\n",
    "SCHEMA_CITY = Namespace(\"https://schema.org/City#\")\n",
    "F1 = Namespace(\"http://www.dei.unipd.it/database2/Formula1Ontology#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CSV files that maps between denominations and country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom_csv = base_path + '/data/utils/denom.csv'\n",
    "denom_df = pd.read_csv(denom_csv, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes_csv = base_path + '/data/utils/iso_country_codes.csv'\n",
    "country_codes_df = pd.read_csv(country_codes_csv, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import circuit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"cities\", SCHEMA_CITY)\n",
    "g.bind(\"f1\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_csv = base_path + '/data/circuits.csv'\n",
    "circuits_df = pd.read_csv(circuits_csv, sep=',', index_col='circuitId')\n",
    "circuits_df = circuits_df.replace({np.nan: None, '\\\\N': None})\n",
    "circuits_df['location'] = circuits_df['location'].str.replace(' ','')     #remove white spaces from city names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the circuit's data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in circuits_df.iterrows():\n",
    "    idC = \"circuit\"+str(index)  # create a unique identifier for the circuit\n",
    "    Circuit = URIRef(F1[idC])   # create the circuit\n",
    "\n",
    "    g.add((Circuit, RDF.type, F1.Circuit))\n",
    "    g.add((Circuit, F1['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "\n",
    "    #add altitude info\n",
    "    if not row['alt'] == None:\n",
    "        g.add((Circuit, F1['altitude'], Literal(row['alt'], datatype=XSD.int)))\n",
    "\n",
    "    #add latitude info\n",
    "    g.add((Circuit, F1['latitude'], Literal(row['lat'], datatype=XSD.decimal)))\n",
    "\n",
    "    #add longitude info\n",
    "    g.add((Circuit, F1['longitude'], Literal(row['lng'], datatype=XSD.decimal)))\n",
    "\n",
    "    #add circuit city\n",
    "    City = URIRef(SCHEMA_CITY[row[\"location\"]])\n",
    "    g.add((Circuit, F1['hasCity'], City))\n",
    "\n",
    "    # Convert the country to the associated alpha-2 code\n",
    "    country = country_codes_df.loc[country_codes_df['name'] == row['country']]\n",
    "\n",
    "    if not country.empty:\n",
    "        country_code = country['alpha-2'].iloc[0]\n",
    "        Country = URIRef(CNS[country_code])\n",
    "        g.add((Circuit, F1['hasCountry'], Country))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the serialized graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/circuits.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import constructor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"f1\", F1)\n",
    "g.bind(\"dbpedia_f1\", DBPEDIA_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructors_csv = base_path + '/data/constructors.csv'\n",
    "constructors_df = pd.read_csv(constructors_csv, sep=',', index_col='constructorId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load constructor participation: associate constructor nationality to country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructors_df = pd.merge(constructors_df, denom_df, on='nationality', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load constructor's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_standings_csv = base_path + '/data/constructor_standings.csv'\n",
    "constructor_standings_df = pd.read_csv(constructor_standings_csv, sep=',', index_col='constructorStandingsId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_results_csv = base_path + '/data/constructor_results.csv'\n",
    "constructor_results_df = pd.read_csv(constructor_results_csv, sep=',', index_col='constructorResultsId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the constructor's data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "for constructorId, row in constructors_df.iterrows():\n",
    "    idCons = \"constructor\"+str(constructorId)   #unique identifier for the constructor\n",
    "    FormulaOneTeam = URIRef(DBPEDIA_F1[idCons])\n",
    "\n",
    "    g.add((FormulaOneTeam, RDF.type, DBPEDIA_F1.FormulaOneTeam))\n",
    "    g.add((FormulaOneTeam, F1['name'], Literal(row['name'], datatype=XSD.string)))\n",
    "\n",
    "    # Convert the country to the associated alpha-2 code\n",
    "    country = country_codes_df.loc[country_codes_df['name'] == row['country']]\n",
    "\n",
    "    if not country.empty:\n",
    "        country_code = country['alpha-2'].iloc[0]\n",
    "        Country = URIRef(CNS[country_code])\n",
    "        g.add((FormulaOneTeam, F1['hasCountry'], Country))\n",
    "\n",
    "    # Find all the races in which a constructor has participated\n",
    "    participatedRaces = constructor_results_df.loc[constructor_results_df['constructorId'] == constructorId]\n",
    "\n",
    "    for participationId, row in participatedRaces.iterrows():\n",
    "        idPart = \"participation\"+str(participationId)\n",
    "        Participate = URIRef(F1[idPart])\n",
    "        g.add((Participate, RDF.type, F1.Participate))\n",
    "\n",
    "        #get data from constructor standings for the given race\n",
    "        constructor_standing = constructor_standings_df.loc[(constructor_standings_df['raceId'] == row['raceId']) & (constructor_standings_df['constructorId'] == constructorId)]\n",
    "\n",
    "        if not constructor_standing.empty:\n",
    "            g.add((Participate, F1['cp_points_after_race'], Literal(constructor_standing['points'].iloc[0], datatype=XSD.int)))\n",
    "            g.add((Participate, F1['cp_position_after_race'], Literal(constructor_standing['position'].iloc[0], datatype=XSD.int)))\n",
    "            g.add((Participate, F1['number_of_wins_after_race'], Literal(constructor_standing['wins'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "        #add the race_weekend associated to the drive\n",
    "        idRWE = \"raceWeekEnd\"+str(row['raceId'])\n",
    "        g.add((Participate, F1['during'], URIRef(F1[idRWE])))\n",
    "\n",
    "        #add the drive\n",
    "        g.add((FormulaOneTeam, F1['partecipateIn'], Participate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/constructors.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import driver data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph and bind the ontologies namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"person\", W3ID_PERSON)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"countries\", CNS)\n",
    "g.bind(\"f1\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_csv = base_path + '/data/drivers.csv'                               \n",
    "driver_df = pd.read_csv(drivers_csv, sep=',')\n",
    "driver_df = driver_df.replace({np.nan: None, '\\\\N': None})\n",
    "driver_df = pd.merge(driver_df, denom_df, on='nationality', how='inner')    #associate the nationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers race results from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv = base_path + '/data/results.csv'\n",
    "results_df = pd.read_csv(results_csv, sep=',', index_col='resultId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers qualifying results from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying_csv = base_path + '/data/qualifying.csv'\n",
    "qualifying_df = pd.read_csv(qualifying_csv, sep=',', index_col='qualifyId')\n",
    "qualifying_df = qualifying_df.replace({np.nan: None, '\\\\N': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the drivers sprint race results from the csv file (Warning: not all the race week ends have a sprint race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprint_races_csv = base_path + '/data/sprint_results.csv'\n",
    "sprint_races_df = pd.read_csv(sprint_races_csv, sep=',', index_col='resultId')\n",
    "sprint_races_df = sprint_races_df.replace({np.nan: None, '\\\\N': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the driver championship standing from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_standings_csv = base_path + '/data/driver_standings.csv'\n",
    "driver_standings_df = pd.read_csv(driver_standings_csv, sep=',', index_col='driverStandingsId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the race status from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_csv = base_path + '/data/status.csv'\n",
    "status_df = pd.read_csv(status_csv, sep=',', index_col='statusId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create the drivers and associate all the informations\n",
    "2) Create the drives related to the drivers and associate all the informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using a compiled regex matches string of type MM:SS.FFFF that represents lap times.\n",
    "If no match is possible returns NaN\n",
    "'''\n",
    "\n",
    "time_tokenizer = re.compile(\"[: .]\")\n",
    "def str_time_to_decimal(time: str) -> float:\n",
    "    tokens = time_tokenizer.split(time)\n",
    "\n",
    "    if len(tokens) != 3: \n",
    "        return float('NaN')\n",
    "        \n",
    "    time = 0.0\n",
    "    time += int(tokens[0]) * 60 + float(tokens[1]) + float(\"0.\" + tokens[2])\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "for driverId, row in driver_df.iterrows():\n",
    "    idD = \"driver\"+str(row['driverId'])\n",
    "    Driver = URIRef(F1[idD])\n",
    "    \n",
    "    #Driver is a subclass of Person\n",
    "    g.add((F1.Driver, RDFS.subClassOf, W3ID_PERSON.Person))\n",
    "    g.add((Driver, RDF.type, F1.Driver))\n",
    "    \n",
    "    #Add first_name, last_name and birth_date (Person data properties) to Driver\n",
    "    g.add((Driver, W3ID_PERSON['first_name'], Literal(row['forename'], datatype=XSD.string)))\n",
    "    g.add((Driver, W3ID_PERSON['last_name'], Literal(row['surname'], datatype=XSD.string)))\n",
    "    g.add((Driver, W3ID_PERSON['birth_date'], Literal(row['dob'], datatype=XSD.date)))\n",
    "\n",
    "    #Add driver number and code\n",
    "    if row['number'] != None:\n",
    "        g.add((Driver, F1['number'], Literal(row['number'], datatype=XSD.int)))\n",
    "    \n",
    "    if row['code'] != None:\n",
    "        g.add((Driver, F1['code'], Literal(row['code'], datatype=XSD.string)))       \n",
    "\n",
    "    #Add driver country\n",
    "\n",
    "    # Convert the country to the associated alpha-2 code\n",
    "    country = country_codes_df.loc[country_codes_df['name'] == row['country']]\n",
    "\n",
    "    if not country.empty:\n",
    "        country_code = country['alpha-2'].iloc[0]\n",
    "        Country = URIRef(CNS[country_code])\n",
    "        g.add((Driver, W3ID_PERSON['nationality'], Country))\n",
    "\n",
    "    #get all the race, qualifying, sprint race results of the given driver, these are the driver's drives\n",
    "    driverDrives = results_df.loc[results_df['driverId'] == driverId] \n",
    "    \n",
    "    #add the race status information\n",
    "    driverDrives = pd.merge(driverDrives, status_df, on='statusId', how='inner')\n",
    "\n",
    "    for driveId, row in driverDrives.iterrows():\n",
    "        idDrive = \"drive\"+str(driveId)\n",
    "\n",
    "        #create the Drive object, a Drive contains all the race weekend performance information of a given driver\n",
    "        Drive = URIRef(F1[idDrive])\n",
    "\n",
    "        #id of the constructor for which the driver drives\n",
    "        idCons = \"constructor\"+str(row['constructorId'])\n",
    "        g.add((Drive, F1['driveFor'], URIRef(F1[idCons])))\n",
    "\n",
    "        g.add((Drive, RDF.type, F1.Drive))\n",
    "        g.add((Drive, F1['race_position'], Literal(row['positionOrder'], datatype=XSD.string))) \n",
    "        g.add((Drive, F1['race_fastest_lap'], Literal(row['fastestLapTime'], datatype=XSD.string)))\n",
    "        \n",
    "        #add race status\n",
    "        g.add((Drive, F1['status'], Literal(row['status'], datatype=XSD.string)))\n",
    "\n",
    "        #searching the qualify of the given driver in the given drive \n",
    "        qualifyDriver = qualifying_df.loc[(qualifying_df['driverId'] == driverId) & (qualifying_df['raceId'] == row['raceId'])]\n",
    "\n",
    "        if not qualifyDriver.empty:\n",
    "            g.add((Drive, F1['quali_position'], Literal(qualifyDriver['position'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "            #check for qualify times\n",
    "            if qualifyDriver['q1'].iloc[0] != None:\n",
    "                time = str_time_to_decimal(qualifyDriver['q1'].iloc[0])\n",
    "                g.add((Drive, F1['q1_time'], Literal(time, datatype=XSD.decimal)))\n",
    "            \n",
    "            if qualifyDriver['q2'].iloc[0] != None:\n",
    "                time = str_time_to_decimal(qualifyDriver['q2'].iloc[0])\n",
    "                g.add((Drive, F1['q2_time'], Literal(time, datatype=XSD.decimal)))\n",
    "\n",
    "            if qualifyDriver['q3'].iloc[0] != None:\n",
    "                time = str_time_to_decimal(qualifyDriver['q3'].iloc[0])\n",
    "                g.add((Drive, F1['q3_time'], Literal(time, datatype=XSD.decimal)))\n",
    "\n",
    "        #searching the sprint_race position of the given driver in the given drive if present\n",
    "        sprintDriver = sprint_races_df.loc[(sprint_races_df['driverId'] == driverId) & (sprint_races_df['raceId'] == row['raceId'])]\n",
    "\n",
    "        #check for sprint_race position\n",
    "        if not sprintDriver.empty:\n",
    "            g.add((Drive, F1['sprint_position'], Literal(sprintDriver['position'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "        sprintDriver = sprint_races_df.loc[(sprint_races_df['driverId'] == driverId) & (sprint_races_df['raceId'] == row['raceId'])]\n",
    "\n",
    "        #get data from constructor standings for the given race\n",
    "        driver_standing = driver_standings_df.loc[(driver_standings_df['raceId'] == row['raceId']) & (driver_standings_df['driverId'] == driveId)]\n",
    "\n",
    "        if not driver_standing.empty:\n",
    "            g.add((Drive, F1['cp_points_after_race'], Literal(driver_standing['points'].iloc[0], datatype=XSD.int)))\n",
    "            g.add((Drive, F1['cp_position_after_race'], Literal(driver_standing['position'].iloc[0], datatype=XSD.int)))\n",
    "            g.add((Drive, F1['number_of_wins'], Literal(driver_standing['wins'].iloc[0], datatype=XSD.int)))\n",
    "\n",
    "\n",
    "        #TODO: Race fastest lap\n",
    "        #TODO: Pit stop count\n",
    "        #TODO: Longest pit stop\n",
    "        #TODO: Fastest pit stop\n",
    "\n",
    "        #add the race_weekend associated to the drive\n",
    "        idRWE = \"raceWeekEnd\"+str(row['raceId'])\n",
    "        g.add((Drive, F1['during'], URIRef(F1[idRWE])))  \n",
    "\n",
    "        #add the drive\n",
    "        g.add((Driver, F1['hasDrivenIn'], Drive))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/drivers.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Race Weekends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"f1\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the race weekends from the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_csv = base_path + '/data/races.csv'   #Read the races\n",
    "races_df = pd.read_csv(races_csv, sep=',', index_col='raceId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the fans rating for the race weekends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_ratings_csv = base_path + '/data/fan_ratings.csv'   #Read the races\n",
    "fan_ratings_df = pd.read_csv(fan_ratings_csv, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to be able to merge\n",
    "fan_ratings_df.rename(columns={'Y': 'year', 'R': 'round','RATING':'rating'}, inplace=True)\n",
    "fan_rated_races = pd.merge(races_df, fan_ratings_df, how='left', on=['year', 'round'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raceId, row in fan_rated_races.iterrows():\n",
    "    idRWE = \"raceWeekEnd\"+str(raceId)\n",
    "    Race = URIRef(F1[idRWE])\n",
    "    \n",
    "    g.add((Race, RDF.type, F1.RaceWeekend))\n",
    "    g.add((Race, F1['name'], Literal(row['name'], datatype=XSD.string))) \n",
    "    g.add((Race, F1['year'], Literal(row['year'], datatype=XSD.int)))\n",
    "    g.add((Race, F1['round'], Literal(row['round'], datatype=XSD.int)))\n",
    "    g.add((Race, F1['date'], Literal(row['date'], datatype=XSD.date)))\n",
    "\n",
    "    if not np.isnan(row['rating']):\n",
    "        g.add((Race, F1['fans_rating'], Literal(row['rating'], datatype=XSD.float)))\n",
    "\n",
    "    #create circuit node\n",
    "    idC = 'circuit' + str(row['circuitId'])\n",
    "    Circuit = URIRef(F1[idC])\n",
    "\n",
    "    g.add((Race, F1['takePlaceIn'], Circuit))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + '/rdf/raceWeekend.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('db2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019df42223066dde4157690601c8100b607ef52bb43ff483069c00805dbb71b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
